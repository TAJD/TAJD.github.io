I"∆	<p>These are my crib notes for the key linear algebra topics for the <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">MIT Opencourseware course Matrix Methods in Data Analysis, Signal Processing, and Machine Learning.</a> The accompanying text for this course is <a href="https://math.mit.edu/~gs/learningfromdata/">Linear algebra and learning from Data</a> which I think is a rather good breeze through all the things one needs to know to do data science. I take no responsibility for the ideas on this page, they are all the fault of the sources that I‚Äôve linked to.</p>

<h1 id="summary">Summary</h1>

<p>Linear algebra has many different matrices, but positive definite symmetric matrices $ S $ are the best ones as they have positive eigenvalues $ \lambda $ and orthogonal eigenvectors $ q $. $ S $ matrices are combinations $ S = \lambda_1 q_1 q_1^T + \lambda_2 q_2 q_2^T ‚Ä¶ $ of simple rank-one projections $qq^T$ onto those eigenvectors.</p>

<p>If $\lambda_1 \geq \lambda_2 \geq \lambda_2 ‚Ä¶ $ then $ \lambda_1 q_1 q_1^T$ is the most informative part of $S$. For a sample covariance matrix this part has the greatest variation.</p>

<p>These ideas are extended from symmetric matrices to all matrices using the singular value decomposition (SVD). The matrix $A$ is decomposed into two sets of singular vectors $u$s and $v$s and singular values $\sigma$ replace eigenvalues $\lambda$.</p>

<script type="math/tex; mode=display">A = \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + ...</script>

<script type="math/tex; mode=display">A = U \Sigma V^T</script>

<p>With decreasing $ \sigma $‚Äôs those rank-one pieces of $A$ still come in order of importance. The ‚ÄúEckart-Young Theorem‚Äù about $A$ complements what we know about the symmetric matrix $A^T A$: for rank $k$, stop at $ \sigma_k u_k v_k^T$.</p>

<h1 id="topics-covered-in-p1-of-lada">Topics covered in P1 of LADA</h1>

<ol>
  <li>Multiplication $Ax$ using columns of $A$</li>
  <li>Matrix-Matrix multiplication $ \boldsymbol{AB}$</li>
  <li>The four fundamental subspaces</li>
  <li>Elimination and $ \boldsymbol{A = LU} $</li>
  <li>Orthogonal matrices and subspaces</li>
  <li>Eigenvalues and eigenvalues</li>
  <li>Symmetric positive definite matrices</li>
  <li>Singular values and singular vectors in the SVD</li>
  <li>Principal components and the best low rank matrix</li>
  <li>Norms of vectors and functions and matrices</li>
</ol>
:ET